{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8642251,
     "sourceType": "datasetVersion",
     "datasetId": 5175864
    },
    {
     "sourceId": 8674749,
     "sourceType": "datasetVersion",
     "datasetId": 5199500
    }
   ],
   "dockerImageVersionId": 30732,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas numpy scikit-learn transformers tokenizers mlflow databricks-cli"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-13T19:29:56.005111Z",
     "iopub.execute_input": "2024-06-13T19:29:56.006004Z",
     "iopub.status.idle": "2024-06-13T19:30:13.906701Z",
     "shell.execute_reply.started": "2024-06-13T19:29:56.005941Z",
     "shell.execute_reply": "2024-06-13T19:30:13.905740Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.19.1)\nCollecting mlflow\n  Downloading mlflow-2.13.2-py3-none-any.whl.metadata (29 kB)\nCollecting databricks-cli\n  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.1)\nCollecting cachetools<6,>=5.0.0 (from mlflow)\n  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.0.0)\nRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.41)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.11.0)\nRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.5.2)\nRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\nRequirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (14.0.2)\nCollecting querystring-parser<2 (from mlflow)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.25)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\nRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\nCollecting gunicorn<23 (from mlflow)\n  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: pyjwt>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli) (2.8.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli) (3.2.2)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from databricks-cli) (0.9.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from databricks-cli) (1.16.0)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /opt/conda/lib/python3.10/site-packages (from databricks-cli) (1.26.18)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nCollecting aniso8601<10,>=8 (from graphene<4->mlflow)\n  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.1)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.43b0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\nDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m25.0/25.0 MB\u001B[0m \u001B[31m55.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hDownloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m150.3/150.3 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\nDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m128.2/128.2 kB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.4/84.4 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.8/52.8 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.9/202.9 kB\u001B[0m \u001B[31m14.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nInstalling collected packages: aniso8601, querystring-parser, graphql-core, cachetools, gunicorn, graphql-relay, databricks-cli, graphene, mlflow\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 4.2.4\n    Uninstalling cachetools-4.2.4:\n      Successfully uninstalled cachetools-4.2.4\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed aniso8601-9.0.1 cachetools-5.3.2 databricks-cli-0.18.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 mlflow-2.13.2 querystring-parser-1.2.4\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import mlflow"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:13.908675Z",
     "iopub.execute_input": "2024-06-13T19:30:13.908996Z",
     "iopub.status.idle": "2024-06-13T19:30:20.168502Z",
     "shell.execute_reply.started": "2024-06-13T19:30:13.908958Z",
     "shell.execute_reply": "2024-06-13T19:30:20.167711Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client(project='winter-origin-387706')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:20.169521Z",
     "iopub.execute_input": "2024-06-13T19:30:20.169996Z",
     "iopub.status.idle": "2024-06-13T19:30:25.183637Z",
     "shell.execute_reply.started": "2024-06-13T19:30:20.169954Z",
     "shell.execute_reply": "2024-06-13T19:30:25.182716Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def download_and_load_csv(bucket_name, file_path, saved_path):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "    blob.download_to_filename(saved_path)\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "bucket_name = 'tweet-sentiment-extraction-mlops'\n",
    "file_path1 = 'train_folds.csv'\n",
    "file_path2 = 'test.csv'\n",
    "\n",
    "train_folds = download_and_load_csv(bucket_name, file_path1, file_path1)\n",
    "test = download_and_load_csv(bucket_name, file_path2, file_path2)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:25.184831Z",
     "iopub.execute_input": "2024-06-13T19:30:25.185369Z",
     "iopub.status.idle": "2024-06-13T19:30:28.209920Z",
     "shell.execute_reply.started": "2024-06-13T19:30:25.185340Z",
     "shell.execute_reply": "2024-06-13T19:30:28.208897Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "bucket_name = 'bert-uncased'\n",
    "folder_path='bert-base-uncased'\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "local_dir = '/kaggle/working/bert_base_uncased/'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=folder_path)  \n",
    "for blob in blobs:\n",
    "    full_path = blob.name\n",
    "    filename = os.path.basename(full_path)\n",
    "    blob.download_to_filename(f'/kaggle/working/bert_base_uncased/{filename}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:28.212605Z",
     "iopub.execute_input": "2024-06-13T19:30:28.212895Z",
     "iopub.status.idle": "2024-06-13T19:30:44.711320Z",
     "shell.execute_reply.started": "2024-06-13T19:30:28.212870Z",
     "shell.execute_reply": "2024-06-13T19:30:44.710421Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "file_path3 = '.databrickscfg'\n",
    "bucket_name = 'tweet-sentiment-extraction-mlops'\n",
    "\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(file_path3)\n",
    "\n",
    "blob.download_to_filename('/.databrickscfg')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:44.712463Z",
     "iopub.execute_input": "2024-06-13T19:30:44.712757Z",
     "iopub.status.idle": "2024-06-13T19:30:45.381891Z",
     "shell.execute_reply.started": "2024-06-13T19:30:44.712731Z",
     "shell.execute_reply": "2024-06-13T19:30:45.380827Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%sh \n",
    "echo \"[DEFAULT]\n",
    "host = https://community.cloud.databricks.com/\n",
    "username = varenyk.pn@ucu.edu.ua\n",
    "password = sK_z7LbRu!JLzyT\n",
    "jobs-api-version = 2.0'\" > ~/.databrickscfg"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:45.383272Z",
     "iopub.execute_input": "2024-06-13T19:30:45.383579Z",
     "iopub.status.idle": "2024-06-13T19:30:45.403251Z",
     "shell.execute_reply.started": "2024-06-13T19:30:45.383553Z",
     "shell.execute_reply": "2024-06-13T19:30:45.402484Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "exp_name = '/Users/varenyk.pn@ucu.edu.ua/experiments-sentiment-analysis2'\n",
    "mlflow.set_tracking_uri('databricks')\n",
    "mlflow.set_experiment(exp_name)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:45.404356Z",
     "iopub.execute_input": "2024-06-13T19:30:45.404676Z",
     "iopub.status.idle": "2024-06-13T19:30:46.778834Z",
     "shell.execute_reply.started": "2024-06-13T19:30:45.404647Z",
     "shell.execute_reply": "2024-06-13T19:30:46.777942Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/1797089850547321', creation_time=1718290663156, experiment_id='1797089850547321', last_update_time=1718300671329, lifecycle_stage='active', name='/Users/varenyk.pn@ucu.edu.ua/experiments-sentiment-analysis2', tags={'mlflow.experiment.sourceName': '/Users/varenyk.pn@ucu.edu.ua/experiments-sentiment-analysis2',\n 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n 'mlflow.ownerEmail': 'varenyk.pn@ucu.edu.ua',\n 'mlflow.ownerId': '2666115447642619'}>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score\n",
    "\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:46.780293Z",
     "iopub.execute_input": "2024-06-13T19:30:46.780609Z",
     "iopub.status.idle": "2024-06-13T19:30:46.795213Z",
     "shell.execute_reply.started": "2024-06-13T19:30:46.780582Z",
     "shell.execute_reply": "2024-06-13T19:30:46.794385Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"./bert_base_uncased\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TRAINING_FILE = \"/kaggle/working/train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:46.796198Z",
     "iopub.execute_input": "2024-06-13T19:30:46.796496Z",
     "iopub.status.idle": "2024-06-13T19:30:46.835445Z",
     "shell.execute_reply.started": "2024-06-13T19:30:46.796463Z",
     "shell.execute_reply": "2024-06-13T19:30:46.834630Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    len_st = len(selected_text)\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "        if tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids[1:-1]\n",
    "    tweet_offsets = tok_tweet.offsets[1:-1]\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 3893,\n",
    "        'negative': 4997,\n",
    "        'neutral': 8699\n",
    "    }\n",
    "    \n",
    "    input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n",
    "    token_type_ids = [0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 3\n",
    "    targets_end += 3\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:46.836519Z",
     "iopub.execute_input": "2024-06-13T19:30:46.836776Z",
     "iopub.status.idle": "2024-06-13T19:30:46.950543Z",
     "shell.execute_reply.started": "2024-06-13T19:30:46.836753Z",
     "shell.execute_reply": "2024-06-13T19:30:46.949723Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len  \n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:46.951578Z",
     "iopub.execute_input": "2024-06-13T19:30:46.951875Z",
     "iopub.status.idle": "2024-06-13T19:30:46.963776Z",
     "shell.execute_reply.started": "2024-06-13T19:30:46.951841Z",
     "shell.execute_reply": "2024-06-13T19:30:46.963019Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        out = torch.cat((outputs.hidden_states[-1], outputs.hidden_states[-2]), dim=-1)\n",
    "        out = self.drop_out(out)\n",
    "        logits = self.l0(out)\n",
    "\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:46.964800Z",
     "iopub.execute_input": "2024-06-13T19:30:46.965081Z",
     "iopub.status.idle": "2024-06-13T19:30:48.678027Z",
     "shell.execute_reply.started": "2024-06-13T19:30:46.965059Z",
     "shell.execute_reply": "2024-06-13T19:30:48.677240Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:48.682728Z",
     "iopub.execute_input": "2024-06-13T19:30:48.683023Z",
     "iopub.status.idle": "2024-06-13T19:30:48.687872Z",
     "shell.execute_reply.started": "2024-06-13T19:30:48.682998Z",
     "shell.execute_reply": "2024-06-13T19:30:48.687007Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:48.689068Z",
     "iopub.execute_input": "2024-06-13T19:30:48.689379Z",
     "iopub.status.idle": "2024-06-13T19:30:48.705173Z",
     "shell.execute_reply.started": "2024-06-13T19:30:48.689349Z",
     "shell.execute_reply": "2024-06-13T19:30:48.704336Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device, epoch):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "            \n",
    "    mlflow.log_metrics({\n",
    "        \"loss\": losses.avg,\n",
    "        \"jaccard\": jaccards.avg \n",
    "    }, step=epoch)        \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:48.706221Z",
     "iopub.execute_input": "2024-06-13T19:30:48.706453Z",
     "iopub.status.idle": "2024-06-13T19:30:48.723694Z",
     "shell.execute_reply.started": "2024-06-13T19:30:48.706432Z",
     "shell.execute_reply": "2024-06-13T19:30:48.722948Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def execute(fold):\n",
    "    dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = TweetModel(conf=model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=2, mode=\"max\")\n",
    "    print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        jaccard = eval_fn(valid_data_loader, model, device, epoch)\n",
    "        print(f\"Jaccard Score = {jaccard}\")\n",
    "        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:48.724816Z",
     "iopub.execute_input": "2024-06-13T19:30:48.725228Z",
     "iopub.status.idle": "2024-06-13T19:30:48.738057Z",
     "shell.execute_reply.started": "2024-06-13T19:30:48.725204Z",
     "shell.execute_reply": "2024-06-13T19:30:48.737261Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for fold in range(0, 5): \n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.set_tag(\"model\", \"Tweeter sentiment extraction with BERT\")\n",
    "        mlflow.log_param(\"model_name\", f'bert-base-uncased_{fold}')\n",
    "\n",
    "        model = execute(fold=fold)\n",
    "\n",
    "        mlflow.pytorch.log_model(model, f\"models/fold_{fold}\")\n",
    "        print(mlflow.get_artifact_uri())\n",
    "\n",
    "    mlflow.end_run()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T19:30:48.739034Z",
     "iopub.execute_input": "2024-06-13T19:30:48.739291Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Training is Starting for fold=0\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe0f340e9c9a40648a75dade5bed7395"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60193a038c4e4e02a778f3f9c6192b4e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Jaccard = 0.6901082298892526\nJaccard Score = 0.6901082298892526\nValidation score improved (-inf --> 0.6901082298892526). Saving model!\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f2c081a2afc47e69bd7e7d30b6af942"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "224f695e5f4042cb94c6d3e109f50fb5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Jaccard = 0.6903415863676274\nJaccard Score = 0.6903415863676274\nEarlyStopping counter: 1 out of 2\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24be4a62516340d59e401dc19e72eca3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca8a5fb8bef74ce9a4f4cf908511d6ef"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Jaccard = 0.685366030257246\nJaccard Score = 0.685366030257246\nEarlyStopping counter: 2 out of 2\nEarly stopping\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n2024/06/13 19:55:01 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.13.2/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f042f814999d415d806b06137ae5fbce"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "dbfs:/databricks/mlflow-tracking/1797089850547321/e0feaf2d6c5c4c718be62f881bb5450c/artifacts\nTraining is Starting for fold=1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ea38cfad0da4900a3cfd1f24b1baa4b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "844a20d2a4b144e9a4f084700d187a0c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Jaccard = 0.7137721165434489\nJaccard Score = 0.7137721165434489\nValidation score improved (-inf --> 0.7137721165434489). Saving model!\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a62b00bf2454b40894077dfa22a05db"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62d0a361ce5d40b4bdf35c59632e9d2c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Jaccard = 0.7168952016234176\nJaccard Score = 0.7168952016234176\nValidation score improved (0.7137721165434489 --> 0.7168952016234176). Saving model!\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/344 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ed94f7c02c145abba7d543e4d38e8f9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "run(fold=1)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run(fold=2)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run(fold=3)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "run(fold=4)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n",
    "df_test.loc[:, \"selected_text\"] = df_test.text.values"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# post-process trick:\n",
    "# Note: This trick comes from: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942\n",
    "def post_process(selected):\n",
    "    return \" \".join(set(selected.lower().split()))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
